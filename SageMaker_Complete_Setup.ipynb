{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GL RL Model - Complete SageMaker Setup\n",
    "\n",
    "This notebook uses the working conda approach to install all dependencies including sentencepiece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install problematic packages using conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sentencepiece and pyarrow from conda-forge (prebuilt binaries)\n",
    "%conda install -c conda-forge sentencepiece pyarrow -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install PyTorch and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch (CPU version for ml.t2.medium)\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Transformers ecosystem\n",
    "%pip install transformers tokenizers huggingface-hub accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install fine-tuning libraries\n",
    "%pip install peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install remaining utilities\n",
    "%pip install numpy pandas protobuf tqdm fsspec aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all imports\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\\n\")\n",
    "\n",
    "# Import all packages\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import peft\n",
    "import trl\n",
    "import pyarrow\n",
    "import sentencepiece\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"âœ… All packages imported successfully!\\n\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"Datasets: {datasets.__version__}\")\n",
    "print(f\"PEFT: {peft.__version__}\")\n",
    "print(f\"TRL: {trl.__version__}\")\n",
    "print(f\"PyArrow: {pyarrow.__version__}\")\n",
    "print(f\"Sentencepiece: {sentencepiece.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "\n",
    "print(f\"\\nDevice: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Qwen Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Test loading the Qwen tokenizer (requires sentencepiece)\n",
    "model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "print(f\"Loading tokenizer for {model_name}...\\n\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "    # Test tokenization\n",
    "    test_queries = [\n",
    "        \"SELECT * FROM users WHERE age > 25;\",\n",
    "        \"Show me all customers\",\n",
    "        \"Calculate total revenue by month\"\n",
    "    ]\n",
    "    \n",
    "    print(\"âœ… Qwen tokenizer loaded successfully!\\n\")\n",
    "    print(f\"Vocabulary size: {tokenizer.vocab_size}\\n\")\n",
    "    \n",
    "    for query in test_queries:\n",
    "        tokens = tokenizer.encode(query)\n",
    "        decoded = tokenizer.decode(tokens)\n",
    "        print(f\"Query: '{query}'\")\n",
    "        print(f\"  Tokens: {len(tokens)}\")\n",
    "        print(f\"  Decoded: '{decoded}'\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading tokenizer: {e}\")\n",
    "    print(\"Please check your internet connection and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clone Repository and Set Up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Navigate to SageMaker directory\n",
    "sagemaker_dir = \"/home/ec2-user/SageMaker\"\n",
    "if os.path.exists(sagemaker_dir):\n",
    "    os.chdir(sagemaker_dir)\n",
    "    print(f\"Changed to directory: {os.getcwd()}\\n\")\n",
    "\n",
    "# Clone or update repository\n",
    "if not os.path.exists(\"gl_rl_model\"):\n",
    "    !git clone https://github.com/maddinenisri/gl_rl_model.git\n",
    "    print(\"Repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"Repository already exists\")\n",
    "\n",
    "os.chdir(\"gl_rl_model\")\n",
    "!git pull origin main\n",
    "\n",
    "print(f\"\\nCurrent directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data\n",
    "os.makedirs(\"data/training\", exist_ok=True)\n",
    "\n",
    "sample_data = [\n",
    "    {\"query\": \"Show me all customers\", \"sql\": \"SELECT * FROM customers;\", \"context\": \"customers(id, name, email, created_at)\"},\n",
    "    {\"query\": \"Get total sales by month\", \"sql\": \"SELECT DATE_FORMAT(date, '%Y-%m') as month, SUM(amount) as total FROM sales GROUP BY month;\", \"context\": \"sales(id, date, amount, product_id)\"},\n",
    "    {\"query\": \"Find top 5 products by revenue\", \"sql\": \"SELECT p.name, SUM(s.amount) as revenue FROM products p JOIN sales s ON p.id = s.product_id GROUP BY p.id ORDER BY revenue DESC LIMIT 5;\", \"context\": \"products(id, name, price), sales(id, product_id, amount)\"},\n",
    "    {\"query\": \"List users who registered today\", \"sql\": \"SELECT * FROM users WHERE DATE(created_at) = CURDATE();\", \"context\": \"users(id, name, email, created_at)\"},\n",
    "    {\"query\": \"Calculate average order value\", \"sql\": \"SELECT AVG(total_amount) as avg_order_value FROM orders;\", \"context\": \"orders(id, customer_id, total_amount, order_date)\"}\n",
    "]\n",
    "\n",
    "data_file = \"data/training/query_pairs.jsonl\"\n",
    "with open(data_file, 'w') as f:\n",
    "    for item in sample_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"âœ… Created training data: {data_file}\\n\")\n",
    "\n",
    "# Load and display\n",
    "with open(data_file, 'r') as f:\n",
    "    loaded_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Loaded {len(loaded_data)} training examples\")\n",
    "for i, example in enumerate(loaded_data[:3], 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Query: {example['query']}\")\n",
    "    print(f\"  SQL: {example['sql'][:50]}...\" if len(example['sql']) > 50 else f\"  SQL: {example['sql']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Data Loading with Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the training data using datasets library\n",
    "try:\n",
    "    dataset = load_dataset('json', data_files='data/training/query_pairs.jsonl', split='train')\n",
    "    print(f\"âœ… Dataset loaded successfully!\")\n",
    "    print(f\"\\nDataset info:\")\n",
    "    print(f\"  Number of examples: {len(dataset)}\")\n",
    "    print(f\"  Features: {dataset.features}\")\n",
    "    print(f\"\\nFirst example:\")\n",
    "    print(f\"  {dataset[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Environment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENVIRONMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# System info\n",
    "print(f\"\\nðŸ“Š System Information:\")\n",
    "print(f\"  Platform: {platform.platform()}\")\n",
    "print(f\"  Python: {platform.python_version()}\")\n",
    "print(f\"  Machine: {platform.machine()}\")\n",
    "\n",
    "# Check if in SageMaker\n",
    "if os.path.exists('/opt/ml/metadata/resource-metadata.json'):\n",
    "    print(f\"\\nðŸš€ SageMaker: Yes\")\n",
    "    try:\n",
    "        with open('/opt/ml/metadata/resource-metadata.json', 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"  Instance Type: {metadata.get('InstanceType', 'Unknown')}\")\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(f\"\\nðŸ’» SageMaker: No (local environment)\")\n",
    "\n",
    "# GPU status\n",
    "print(f\"\\nðŸŽ® GPU Status:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  CUDA Available: Yes\")\n",
    "    print(f\"  GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(f\"  CUDA Available: No (CPU only)\")\n",
    "    print(f\"  Note: Use SageMaker Training Jobs for GPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… SETUP COMPLETE - ALL DEPENDENCIES INSTALLED!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ“š Next Steps:\")\n",
    "print(\"1. Open GL_RL_Model_Quick_Start.ipynb for training\")\n",
    "print(\"2. Use this notebook for development\")\n",
    "print(\"3. Launch GPU training with SageMaker Training Jobs\")\n",
    "print(\"\\nðŸ’¡ Remember to stop the instance when not in use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Success!\n",
    "\n",
    "All dependencies including **sentencepiece** and **pyarrow** are now installed.\n",
    "\n",
    "### Key Points:\n",
    "- Used `conda-forge` for problematic packages (avoids compilation)\n",
    "- Used `pip` for remaining packages\n",
    "- Qwen model tokenizer works with sentencepiece\n",
    "- Ready for training on this CPU instance or GPU via Training Jobs\n",
    "\n",
    "### To reinstall if needed:\n",
    "```python\n",
    "%conda install -c conda-forge sentencepiece pyarrow -y\n",
    "%pip install transformers datasets peft trl accelerate\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}