{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GL RL Model - Complete SageMaker Setup\n",
    "\n",
    "This notebook uses the working conda approach to install all dependencies including sentencepiece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install problematic packages using conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sentencepiece and pyarrow from conda-forge (prebuilt binaries)\n",
    "%conda install -c conda-forge sentencepiece pyarrow -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install PyTorch and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch (CPU version for ml.t2.medium)\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Transformers ecosystem\n",
    "%pip install transformers tokenizers huggingface-hub accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install fine-tuning libraries\n",
    "%pip install peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install remaining utilities\n",
    "%pip install numpy pandas protobuf tqdm fsspec aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all imports\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\\n\")\n",
    "\n",
    "# Import all packages\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import peft\n",
    "import trl\n",
    "import pyarrow\n",
    "import sentencepiece\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"✅ All packages imported successfully!\\n\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"Datasets: {datasets.__version__}\")\n",
    "print(f\"PEFT: {peft.__version__}\")\n",
    "print(f\"TRL: {trl.__version__}\")\n",
    "print(f\"PyArrow: {pyarrow.__version__}\")\n",
    "print(f\"Sentencepiece: {sentencepiece.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "\n",
    "print(f\"\\nDevice: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Qwen Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Test loading the Qwen tokenizer (requires sentencepiece)\n",
    "model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "print(f\"Loading tokenizer for {model_name}...\\n\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "    # Test tokenization\n",
    "    test_queries = [\n",
    "        \"SELECT * FROM users WHERE age > 25;\",\n",
    "        \"Show me all customers\",\n",
    "        \"Calculate total revenue by month\"\n",
    "    ]\n",
    "    \n",
    "    print(\"✅ Qwen tokenizer loaded successfully!\\n\")\n",
    "    print(f\"Vocabulary size: {tokenizer.vocab_size}\\n\")\n",
    "    \n",
    "    for query in test_queries:\n",
    "        tokens = tokenizer.encode(query)\n",
    "        decoded = tokenizer.decode(tokens)\n",
    "        print(f\"Query: '{query}'\")\n",
    "        print(f\"  Tokens: {len(tokens)}\")\n",
    "        print(f\"  Decoded: '{decoded}'\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading tokenizer: {e}\")\n",
    "    print(\"Please check your internet connection and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clone Repository and Set Up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Navigate to SageMaker directory\n",
    "sagemaker_dir = \"/home/ec2-user/SageMaker\"\n",
    "if os.path.exists(sagemaker_dir):\n",
    "    os.chdir(sagemaker_dir)\n",
    "    print(f\"Changed to directory: {os.getcwd()}\\n\")\n",
    "\n",
    "# Clone or update repository\n",
    "if not os.path.exists(\"gl_rl_model\"):\n",
    "    !git clone https://github.com/maddinenisri/gl_rl_model.git\n",
    "    print(\"Repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"Repository already exists\")\n",
    "\n",
    "os.chdir(\"gl_rl_model\")\n",
    "!git pull origin main\n",
    "\n",
    "print(f\"\\nCurrent directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data\n",
    "os.makedirs(\"data/training\", exist_ok=True)\n",
    "\n",
    "sample_data = [\n",
    "    {\"query\": \"Show me all customers\", \"sql\": \"SELECT * FROM customers;\", \"context\": \"customers(id, name, email, created_at)\"},\n",
    "    {\"query\": \"Get total sales by month\", \"sql\": \"SELECT DATE_FORMAT(date, '%Y-%m') as month, SUM(amount) as total FROM sales GROUP BY month;\", \"context\": \"sales(id, date, amount, product_id)\"},\n",
    "    {\"query\": \"Find top 5 products by revenue\", \"sql\": \"SELECT p.name, SUM(s.amount) as revenue FROM products p JOIN sales s ON p.id = s.product_id GROUP BY p.id ORDER BY revenue DESC LIMIT 5;\", \"context\": \"products(id, name, price), sales(id, product_id, amount)\"},\n",
    "    {\"query\": \"List users who registered today\", \"sql\": \"SELECT * FROM users WHERE DATE(created_at) = CURDATE();\", \"context\": \"users(id, name, email, created_at)\"},\n",
    "    {\"query\": \"Calculate average order value\", \"sql\": \"SELECT AVG(total_amount) as avg_order_value FROM orders;\", \"context\": \"orders(id, customer_id, total_amount, order_date)\"}\n",
    "]\n",
    "\n",
    "data_file = \"data/training/query_pairs.jsonl\"\n",
    "with open(data_file, 'w') as f:\n",
    "    for item in sample_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"✅ Created training data: {data_file}\\n\")\n",
    "\n",
    "# Load and display\n",
    "with open(data_file, 'r') as f:\n",
    "    loaded_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Loaded {len(loaded_data)} training examples\")\n",
    "for i, example in enumerate(loaded_data[:3], 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Query: {example['query']}\")\n",
    "    print(f\"  SQL: {example['sql'][:50]}...\" if len(example['sql']) > 50 else f\"  SQL: {example['sql']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Data Loading with Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the training data using datasets library\n",
    "try:\n",
    "    dataset = load_dataset('json', data_files='data/training/query_pairs.jsonl', split='train')\n",
    "    print(f\"✅ Dataset loaded successfully!\")\n",
    "    print(f\"\\nDataset info:\")\n",
    "    print(f\"  Number of examples: {len(dataset)}\")\n",
    "    print(f\"  Features: {dataset.features}\")\n",
    "    print(f\"\\nFirst example:\")\n",
    "    print(f\"  {dataset[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Environment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENVIRONMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# System info\n",
    "print(f\"\\n📊 System Information:\")\n",
    "print(f\"  Platform: {platform.platform()}\")\n",
    "print(f\"  Python: {platform.python_version()}\")\n",
    "print(f\"  Machine: {platform.machine()}\")\n",
    "\n",
    "# Check if in SageMaker\n",
    "if os.path.exists('/opt/ml/metadata/resource-metadata.json'):\n",
    "    print(f\"\\n🚀 SageMaker: Yes\")\n",
    "    try:\n",
    "        with open('/opt/ml/metadata/resource-metadata.json', 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"  Instance Type: {metadata.get('InstanceType', 'Unknown')}\")\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(f\"\\n💻 SageMaker: No (local environment)\")\n",
    "\n",
    "# GPU status\n",
    "print(f\"\\n🎮 GPU Status:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  CUDA Available: Yes\")\n",
    "    print(f\"  GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(f\"  CUDA Available: No (CPU only)\")\n",
    "    print(f\"  Note: Use SageMaker Training Jobs for GPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ SETUP COMPLETE - ALL DEPENDENCIES INSTALLED!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n📚 Next Steps:\")\n",
    "print(\"1. Open GL_RL_Model_Quick_Start.ipynb for training\")\n",
    "print(\"2. Use this notebook for development\")\n",
    "print(\"3. Launch GPU training with SageMaker Training Jobs\")\n",
    "print(\"\\n💡 Remember to stop the instance when not in use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Success!\n",
    "\n",
    "All dependencies including **sentencepiece** and **pyarrow** are now installed.\n",
    "\n",
    "### Key Points:\n",
    "- Used `conda-forge` for problematic packages (avoids compilation)\n",
    "- Used `pip` for remaining packages\n",
    "- Qwen model tokenizer works with sentencepiece\n",
    "- Ready for training on this CPU instance or GPU via Training Jobs\n",
    "\n",
    "### To reinstall if needed:\n",
    "```python\n",
    "%conda install -c conda-forge sentencepiece pyarrow -y\n",
    "%pip install transformers datasets peft trl accelerate\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}