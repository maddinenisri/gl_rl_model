{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Trained Model from SageMaker\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Download the trained model from S3\n",
    "2. Load the fine-tuned LoRA weights\n",
    "3. Test SQL generation with proper prompts\n",
    "4. Evaluate model performance\n",
    "\n",
    "**Latest Successful Training Job**: gl-rl-gpu-20250923-033651"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch transformers peft datasets accelerate boto3 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import boto3\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "import time\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# AWS Configuration\n",
    "AWS_PROFILE = \"personal-yahoo\"\n",
    "REGION = \"us-east-1\"\n",
    "ACCOUNT_ID = \"340350204194\"\n",
    "BUCKET = f\"gl-rl-model-sagemaker-{ACCOUNT_ID}-{REGION}\"\n",
    "\n",
    "# Initialize boto3 session\n",
    "session = boto3.Session(profile_name=AWS_PROFILE, region_name=REGION)\n",
    "s3_client = session.client('s3')\n",
    "\n",
    "print(f\"\\nAWS Configuration:\")\n",
    "print(f\"  Profile: {AWS_PROFILE}\")\n",
    "print(f\"  Region: {REGION}\")\n",
    "print(f\"  Bucket: {BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Model from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the training job name\n",
    "JOB_NAME = \"gl-rl-gpu-20250923-033651\"  # Replace with your job name\n",
    "MODEL_S3_PATH = f\"s3://{BUCKET}/output/{JOB_NAME}/output/model.tar.gz\"\n",
    "\n",
    "print(f\"Model S3 path: {MODEL_S3_PATH}\")\n",
    "\n",
    "# Create local directory\n",
    "model_dir = Path(f\"./models/{JOB_NAME}\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download model\n",
    "print(\"\\nDownloading model from S3...\")\n",
    "model_tar_path = model_dir / \"model.tar.gz\"\n",
    "\n",
    "s3_client.download_file(\n",
    "    Bucket=BUCKET,\n",
    "    Key=f\"output/{JOB_NAME}/output/model.tar.gz\",\n",
    "    Filename=str(model_tar_path)\n",
    ")\n",
    "\n",
    "print(f\"Downloaded model to: {model_tar_path}\")\n",
    "\n",
    "# Extract model\n",
    "print(\"Extracting model...\")\n",
    "with tarfile.open(model_tar_path, 'r:gz') as tar:\n",
    "    tar.extractall(model_dir)\n",
    "\n",
    "print(f\"‚úÖ Model extracted to: {model_dir}\")\n",
    "\n",
    "# List extracted files\n",
    "print(\"\\nExtracted files:\")\n",
    "for file in model_dir.glob(\"*\"):\n",
    "    if file.is_file() and file.name != \"model.tar.gz\":\n",
    "        print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model name (same as used in training)\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "\n",
    "print(f\"Loading base model: {BASE_MODEL}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    trust_remote_code=True,\n",
    "    padding_side='left'\n",
    ")\n",
    "\n",
    "# Set padding token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Base model loaded\")\n",
    "\n",
    "# Load LoRA configuration and model\n",
    "print(\"\\nLoading LoRA weights...\")\n",
    "try:\n",
    "    # Load PEFT config\n",
    "    peft_config = PeftConfig.from_pretrained(str(model_dir))\n",
    "    print(f\"LoRA Config: r={peft_config.r}, alpha={peft_config.lora_alpha}\")\n",
    "    \n",
    "    # Load fine-tuned model\n",
    "    model = PeftModel.from_pretrained(base_model, str(model_dir))\n",
    "    \n",
    "    # Optionally merge LoRA weights for faster inference\n",
    "    # model = model.merge_and_unload()\n",
    "    \n",
    "    print(\"‚úÖ Fine-tuned model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load LoRA weights: {e}\")\n",
    "    print(\"Using base model instead\")\n",
    "    model = base_model\n",
    "\n",
    "# Move to device\n",
    "if device.type == \"cuda\":\n",
    "    model = model.cuda()\n",
    "\n",
    "# Set to eval mode\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\nModel ready on {device}\")\n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Generation Function with Proper Prompt Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql(query, context=\"\", max_new_tokens=150, temperature=0.7, verbose=False):\n",
    "    \"\"\"\n",
    "    Generate SQL from natural language query using the exact training format\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the EXACT format from training\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "You are a SQL expert. Generate SQL queries based on natural language questions.\n",
    "Context: {context}<|im_end|>\n",
    "<|im_start|>user\n",
    "{query}<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"PROMPT:\")\n",
    "        print(prompt)\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    # Decode full output\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Extract SQL after assistant tag\n",
    "    if \"<|im_start|>assistant\" in full_response:\n",
    "        sql = full_response.split(\"<|im_start|>assistant\")[-1]\n",
    "    elif \"assistant\" in full_response:\n",
    "        sql = full_response.split(\"assistant\")[-1]\n",
    "    else:\n",
    "        sql = full_response[len(prompt):]\n",
    "    \n",
    "    # Clean up SQL\n",
    "    sql = sql.replace(\"<|im_end|>\", \"\").strip()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nFULL RESPONSE:\")\n",
    "        print(full_response)\n",
    "        print(\"\\nEXTRACTED SQL:\")\n",
    "        print(sql)\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    return sql, generation_time\n",
    "\n",
    "print(\"‚úÖ SQL generation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test SQL Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple query first (verbose mode to see what's happening)\n",
    "test_query = \"Show me all customers\"\n",
    "test_context = \"customers(id, name, email, created_at)\"\n",
    "\n",
    "print(f\"Test Query: {test_query}\")\n",
    "print(f\"Context: {test_context}\")\n",
    "print()\n",
    "\n",
    "sql, gen_time = generate_sql(test_query, test_context, verbose=True)\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"Generated SQL: {sql}\")\n",
    "print(f\"Generation time: {gen_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries matching training data format\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"Show all customers\",\n",
    "        \"context\": \"customers(id, name, email, created_at)\",\n",
    "        \"expected\": \"SELECT * FROM customers;\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Calculate average order value\",\n",
    "        \"context\": \"orders(id, customer_id, total_amount, order_date)\",\n",
    "        \"expected\": \"SELECT AVG(total_amount) as avg_order_value FROM orders;\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Count orders by customer\",\n",
    "        \"context\": \"orders(id, customer_id, total_amount, order_date), customers(id, name)\",\n",
    "        \"expected\": \"SELECT customer_id, COUNT(*) as order_count FROM orders GROUP BY customer_id;\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Find top 5 products by revenue\",\n",
    "        \"context\": \"products(id, name, category), sales(id, product_id, quantity, price)\",\n",
    "        \"expected\": \"SELECT p.name, SUM(s.quantity * s.price) as revenue FROM products p JOIN sales s ON p.id = s.product_id GROUP BY p.id, p.name ORDER BY revenue DESC LIMIT 5;\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Get employees hired last month\",\n",
    "        \"context\": \"employees(id, name, department, hire_date, salary)\",\n",
    "        \"expected\": \"SELECT * FROM employees WHERE hire_date >= DATE_SUB(CURDATE(), INTERVAL 1 MONTH);\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Testing Multiple Queries\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "for i, test in enumerate(test_queries, 1):\n",
    "    print(f\"\\nüìù Query {i}/{len(test_queries)}\")\n",
    "    print(f\"Question: {test['query']}\")\n",
    "    print(f\"Schema: {test['context']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    sql, gen_time = generate_sql(test['query'], test['context'], verbose=False)\n",
    "    \n",
    "    print(f\"Expected: {test['expected']}\")\n",
    "    print(f\"Generated: {sql}\")\n",
    "    print(f\"Time: {gen_time:.2f}s\")\n",
    "    \n",
    "    # Simple similarity check\n",
    "    is_similar = any(keyword in sql.upper() for keyword in ['SELECT', 'FROM', 'WHERE', 'JOIN', 'GROUP', 'ORDER'])\n",
    "    print(f\"Valid SQL: {'‚úÖ' if is_similar else '‚ùå'}\")\n",
    "    \n",
    "    results.append({\n",
    "        'query': test['query'],\n",
    "        'generated': sql,\n",
    "        'expected': test['expected'],\n",
    "        'time': gen_time,\n",
    "        'valid': is_similar\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "valid_count = sum(1 for r in results if r['valid'])\n",
    "avg_time = sum(r['time'] for r in results) / len(results)\n",
    "\n",
    "print(f\"Total queries: {len(results)}\")\n",
    "print(f\"Valid SQL generated: {valid_count}/{len(results)} ({valid_count/len(results)*100:.1f}%)\")\n",
    "print(f\"Average generation time: {avg_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test with Training Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load actual training data to test\n",
    "training_data_path = \"../../gl_rl_model/data/training/query_pairs.jsonl\"\n",
    "\n",
    "if Path(training_data_path).exists():\n",
    "    print(\"Testing with actual training data format...\\n\")\n",
    "    \n",
    "    with open(training_data_path, 'r') as f:\n",
    "        training_examples = [json.loads(line) for line in f.readlines()[:5]]  # Test first 5\n",
    "    \n",
    "    for i, example in enumerate(training_examples, 1):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(f\"Query: {example['query']}\")\n",
    "        \n",
    "        # Use 'context' or 'reasoning' field\n",
    "        context = example.get('context', example.get('reasoning', ''))\n",
    "        print(f\"Context: {context}\")\n",
    "        print(f\"Expected SQL: {example['sql']}\")\n",
    "        \n",
    "        sql, _ = generate_sql(example['query'], context)\n",
    "        print(f\"Generated SQL: {sql}\")\n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(f\"Training data not found at {training_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test results\n",
    "results_file = f\"test_results_{JOB_NAME}.json\"\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump({\n",
    "        'job_name': JOB_NAME,\n",
    "        'model': BASE_MODEL,\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'device': str(device),\n",
    "        'results': results,\n",
    "        'summary': {\n",
    "            'total_queries': len(results),\n",
    "            'valid_sql': valid_count,\n",
    "            'success_rate': valid_count/len(results)*100,\n",
    "            'avg_generation_time': avg_time\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {results_file}\")\n",
    "\n",
    "# Display training metrics if available\n",
    "metrics_path = model_dir / \"metrics.json\"\n",
    "if metrics_path.exists():\n",
    "    with open(metrics_path) as f:\n",
    "        metrics = json.load(f)\n",
    "    print(\"\\nüìà Training Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Troubleshooting Guide\n",
    "\n",
    "### If the model generates poor SQL:\n",
    "\n",
    "1. **Check the prompt format** - Must match training exactly\n",
    "2. **Verify model weights loaded** - Check for LoRA config files\n",
    "3. **Adjust temperature** - Lower (0.3-0.5) for more deterministic output\n",
    "4. **Increase max_new_tokens** - Some queries need more tokens\n",
    "5. **Check training loss** - Should have decreased during training\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "- **Model outputs conversation instead of SQL**: Prompt format mismatch\n",
    "- **Model outputs generic text**: LoRA weights not loaded properly\n",
    "- **Model outputs incomplete SQL**: Increase max_new_tokens\n",
    "- **Model outputs garbage**: Check if training actually worked (loss > 0)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **More Training**: Train for more epochs (5-10)\n",
    "2. **More Data**: Add more diverse training examples\n",
    "3. **Hyperparameter Tuning**: Adjust learning rate, LoRA rank\n",
    "4. **Larger Model**: Try Qwen2.5-Coder-7B for better performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}