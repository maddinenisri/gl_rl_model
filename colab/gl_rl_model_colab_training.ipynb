{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header_section"
   },
   "source": [
    "# GL RL Model - Comprehensive Training & Verification on Google Colab\n",
    "\n",
    "## 🚀 Overview\n",
    "This notebook implements a complete pipeline for training and verifying the General Ledger Reinforcement Learning Model using Google Colab's GPU resources.\n",
    "\n",
    "### Key Features:\n",
    "- **Model**: Qwen2.5-Coder-7B-Instruct with LoRA fine-tuning\n",
    "- **Training**: Two-phase approach (SFT + GRPO)\n",
    "- **Architecture**: Multi-agent system for SQL generation\n",
    "- **GPU**: Full CUDA optimization for Colab T4/V100/A100\n",
    "\n",
    "### Prerequisites:\n",
    "- Google Colab with GPU runtime (Runtime > Change runtime type > GPU)\n",
    "- Google Drive mounted for persistent storage\n",
    "- Hugging Face account (optional, for model access)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1"
   },
   "source": [
    "## 📦 Section 1: Environment Setup & GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability and specifications\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# GPU Information\n",
    "def check_gpu():\n",
    "    try:\n",
    "        gpu_info = subprocess.check_output(['nvidia-smi'], encoding='utf-8')\n",
    "        print(\"🎮 GPU Information:\")\n",
    "        print(\"=\"*50)\n",
    "        print(gpu_info)\n",
    "        \n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"✅ PyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "            print(f\"📊 Number of GPUs: {torch.cuda.device_count()}\")\n",
    "            print(f\"🏷️ GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "            print(f\"🔧 CUDA Version: {torch.version.cuda}\")\n",
    "        else:\n",
    "            print(\"❌ No GPU available. Please enable GPU in Runtime settings.\")\n",
    "            return False\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking GPU: {e}\")\n",
    "        return False\n",
    "\n",
    "gpu_available = check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "print(\"📦 Installing dependencies...\")\n",
    "\n",
    "# Core dependencies\n",
    "!pip install -q torch==2.7.1 --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers==4.35.0\n",
    "!pip install -q tokenizers>=0.21.2\n",
    "!pip install -q accelerate>=0.24.0\n",
    "!pip install -q peft>=0.6.0\n",
    "!pip install -q bitsandbytes>=0.41.0\n",
    "!pip install -q datasets>=2.14.0\n",
    "!pip install -q trl>=0.7.0\n",
    "\n",
    "# Additional utilities\n",
    "!pip install -q sqlparse>=0.4.4\n",
    "!pip install -q pandas>=2.0.0\n",
    "!pip install -q numpy>=1.24.0\n",
    "!pip install -q scikit-learn>=1.3.0\n",
    "!pip install -q matplotlib>=3.5.0\n",
    "!pip install -q seaborn>=0.12.0\n",
    "!pip install -q plotly>=5.0.0\n",
    "!pip install -q tqdm>=4.65.0\n",
    "!pip install -q ipywidgets>=8.0.0\n",
    "\n",
    "print(\"✅ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create working directories\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Base paths\n",
    "DRIVE_BASE = Path('/content/drive/MyDrive/gl_rl_model')\n",
    "LOCAL_BASE = Path('/content/gl_rl_model')\n",
    "\n",
    "# Create directories\n",
    "directories = [\n",
    "    DRIVE_BASE / 'checkpoints',\n",
    "    DRIVE_BASE / 'logs',\n",
    "    DRIVE_BASE / 'data',\n",
    "    DRIVE_BASE / 'exports',\n",
    "    LOCAL_BASE / 'temp',\n",
    "    LOCAL_BASE / 'cache'\n",
    "]\n",
    "\n",
    "for dir_path in directories:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print(f\"📁 Created working directories:\")\n",
    "print(f\"   Drive: {DRIVE_BASE}\")\n",
    "print(f\"   Local: {LOCAL_BASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repository"
   },
   "outputs": [],
   "source": [
    "# Clone the GL RL Model repository\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# Check if repository exists\n",
    "repo_path = LOCAL_BASE / 'repo'\n",
    "if repo_path.exists():\n",
    "    shutil.rmtree(repo_path)\n",
    "\n",
    "# Clone repository (replace with actual repository URL if available)\n",
    "# For now, we'll create the structure manually\n",
    "repo_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, str(repo_path))\n",
    "sys.path.insert(0, str(LOCAL_BASE))\n",
    "\n",
    "print(\"✅ Repository setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_utilities"
   },
   "outputs": [],
   "source": [
    "# Utility functions for Colab\n",
    "import torch\n",
    "import gc\n",
    "from typing import Dict, Any, Optional\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "class ColabUtils:\n",
    "    \"\"\"Utility functions for Google Colab environment.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_memory_usage() -> Dict[str, float]:\n",
    "        \"\"\"Get current memory usage statistics.\"\"\"\n",
    "        # CPU Memory\n",
    "        cpu_memory = psutil.virtual_memory()\n",
    "        cpu_used = cpu_memory.used / 1e9\n",
    "        cpu_total = cpu_memory.total / 1e9\n",
    "        \n",
    "        # GPU Memory\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_used = torch.cuda.memory_allocated() / 1e9\n",
    "            gpu_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            gpu_percent = (gpu_used / gpu_total) * 100\n",
    "        else:\n",
    "            gpu_used = gpu_total = gpu_percent = 0\n",
    "            \n",
    "        return {\n",
    "            'cpu_used_gb': cpu_used,\n",
    "            'cpu_total_gb': cpu_total,\n",
    "            'cpu_percent': cpu_memory.percent,\n",
    "            'gpu_used_gb': gpu_used,\n",
    "            'gpu_total_gb': gpu_total,\n",
    "            'gpu_percent': gpu_percent\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def clear_memory():\n",
    "        \"\"\"Clear GPU and CPU memory.\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "    \n",
    "    @staticmethod\n",
    "    def auto_batch_size(base_batch_size: int = 8) -> int:\n",
    "        \"\"\"Automatically determine batch size based on available GPU memory.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return 1\n",
    "            \n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        \n",
    "        # Heuristic based on GPU memory\n",
    "        if gpu_memory < 8:  # T4 (16GB)\n",
    "            return max(1, base_batch_size // 4)\n",
    "        elif gpu_memory < 20:  # V100 (16GB)\n",
    "            return max(1, base_batch_size // 2)\n",
    "        else:  # A100 (40GB+)\n",
    "            return base_batch_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def monitor_resources():\n",
    "        \"\"\"Display current resource usage.\"\"\"\n",
    "        usage = ColabUtils.get_memory_usage()\n",
    "        print(\"\\n📊 Resource Monitor:\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"CPU: {usage['cpu_used_gb']:.1f}/{usage['cpu_total_gb']:.1f} GB ({usage['cpu_percent']:.1f}%)\")\n",
    "        print(f\"GPU: {usage['gpu_used_gb']:.1f}/{usage['gpu_total_gb']:.1f} GB ({usage['gpu_percent']:.1f}%)\")\n",
    "\n",
    "# Initialize utilities\n",
    "colab_utils = ColabUtils()\n",
    "colab_utils.monitor_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2"
   },
   "source": [
    "## 📊 Section 2: Data Preparation & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_sample_data"
   },
   "outputs": [],
   "source": [
    "# Create sample training data\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Sample training data for GL SQL generation\n",
    "sample_data = [\n",
    "    {\n",
    "        \"query\": \"Show all active projects\",\n",
    "        \"sql\": \"SELECT Project_Code, Project_Name, Status, Start_Date, End_Date FROM PAC_MNT_PROJECTS WHERE Status = 'Active'\",\n",
    "        \"reasoning\": \"Identified 'active projects' as filtering PAC_MNT_PROJECTS table by Status = 'Active'\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"List companies with their contact information\",\n",
    "        \"sql\": \"SELECT c.Company_Name, c.Company_Code, ct.Contact_Name, ct.Email, ct.Phone FROM SRM_COMPANIES c LEFT JOIN SRM_CONTACTS ct ON c.Company_ID = ct.Company_ID\",\n",
    "        \"reasoning\": \"Need to join SRM_COMPANIES with SRM_CONTACTS to get complete contact information\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Find projects with budget over 100000\",\n",
    "        \"sql\": \"SELECT Project_Code, Project_Name, Budget, Actual_Cost FROM PAC_MNT_PROJECTS WHERE Budget > 100000\",\n",
    "        \"reasoning\": \"Simple filter on PAC_MNT_PROJECTS using Budget column with comparison operator\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Show resource allocation for active projects\",\n",
    "        \"sql\": \"SELECT p.Project_Name, r.Resource_Name, s.Role, s.Allocation_Percent FROM PAC_MNT_PROJECTS p INNER JOIN PROJSTAFF s ON p.Project_Code = s.Project_Code INNER JOIN PAC_MNT_RESOURCES r ON s.Resource_Code = r.Resource_Code WHERE p.Status = 'Active' AND s.Status = 'Assigned'\",\n",
    "        \"reasoning\": \"Three-way join between projects, staff assignments, and resources, filtered by active status\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Count projects per company\",\n",
    "        \"sql\": \"SELECT c.Company_Name, COUNT(DISTINCT ct.Project_Code) as Project_Count FROM SRM_COMPANIES c LEFT JOIN PROJCNTRTS ct ON c.Company_Code = ct.Company_Code GROUP BY c.Company_Name ORDER BY Project_Count DESC\",\n",
    "        \"reasoning\": \"Aggregate query using COUNT with GROUP BY on company, joining through contracts\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save training data\n",
    "data_path = LOCAL_BASE / 'data' / 'training_data.jsonl'\n",
    "data_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(data_path, 'w') as f:\n",
    "    for item in sample_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Also save to Drive\n",
    "drive_data_path = DRIVE_BASE / 'data' / 'training_data.jsonl'\n",
    "with open(drive_data_path, 'w') as f:\n",
    "    for item in sample_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"✅ Created sample training data with {len(sample_data)} examples\")\n",
    "print(f\"📁 Saved to: {data_path}\")\n",
    "print(f\"📁 Backed up to: {drive_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_loader"
   },
   "outputs": [],
   "source": [
    "# Data loading and preprocessing utilities\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Dict, Any, Optional\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Configuration for data loading.\"\"\"\n",
    "    max_seq_length: int = 512\n",
    "    train_split: float = 0.8\n",
    "    val_split: float = 0.1\n",
    "    test_split: float = 0.1\n",
    "    seed: int = 42\n",
    "\n",
    "class GLDataset(Dataset):\n",
    "    \"\"\"Dataset for GL SQL generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Dict[str, str]], tokenizer=None, max_length: int = 512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Format as instruction-following prompt\n",
    "        prompt = self._format_prompt(item['query'])\n",
    "        response = self._format_response(item['sql'], item.get('reasoning', ''))\n",
    "        \n",
    "        if self.tokenizer:\n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            targets = self.tokenizer(\n",
    "                response,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'input_ids': inputs['input_ids'].squeeze(),\n",
    "                'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "                'labels': targets['input_ids'].squeeze(),\n",
    "                'query': item['query'],\n",
    "                'sql': item['sql']\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'prompt': prompt,\n",
    "                'response': response,\n",
    "                'query': item['query'],\n",
    "                'sql': item['sql'],\n",
    "                'reasoning': item.get('reasoning', '')\n",
    "            }\n",
    "    \n",
    "    def _format_prompt(self, query: str) -> str:\n",
    "        \"\"\"Format query as instruction prompt.\"\"\"\n",
    "        return f\"\"\"### Instruction:\n",
    "Generate an SQL query for the following natural language request.\n",
    "Use the General Ledger schema with tables like PAC_MNT_PROJECTS, SRM_COMPANIES, etc.\n",
    "\n",
    "### Query:\n",
    "{query}\n",
    "\n",
    "### SQL:\"\"\"\n",
    "    \n",
    "    def _format_response(self, sql: str, reasoning: str = \"\") -> str:\n",
    "        \"\"\"Format SQL and reasoning as response.\"\"\"\n",
    "        if reasoning:\n",
    "            return f\"\"\"{sql}\n",
    "\n",
    "### Reasoning:\n",
    "{reasoning}\"\"\"\n",
    "        return sql\n",
    "\n",
    "class DatasetLoader:\n",
    "    \"\"\"Loader for managing train/val/test splits.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataConfig = None):\n",
    "        self.config = config or DataConfig()\n",
    "        random.seed(self.config.seed)\n",
    "    \n",
    "    def load_data(self, file_path: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Load data from JSONL file.\"\"\"\n",
    "        data = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    data.append(json.loads(line))\n",
    "        return data\n",
    "    \n",
    "    def split_data(self, data: List[Dict]) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Split data into train/val/test sets.\"\"\"\n",
    "        # Shuffle data\n",
    "        data = data.copy()\n",
    "        random.shuffle(data)\n",
    "        \n",
    "        # Calculate split indices\n",
    "        n = len(data)\n",
    "        train_idx = int(n * self.config.train_split)\n",
    "        val_idx = train_idx + int(n * self.config.val_split)\n",
    "        \n",
    "        return {\n",
    "            'train': data[:train_idx],\n",
    "            'val': data[train_idx:val_idx],\n",
    "            'test': data[val_idx:]\n",
    "        }\n",
    "    \n",
    "    def create_dataloaders(\n",
    "        self,\n",
    "        data_splits: Dict[str, List[Dict]],\n",
    "        tokenizer=None,\n",
    "        batch_size: int = 4\n",
    "    ) -> Dict[str, DataLoader]:\n",
    "        \"\"\"Create DataLoaders for each split.\"\"\"\n",
    "        loaders = {}\n",
    "        \n",
    "        for split_name, split_data in data_splits.items():\n",
    "            dataset = GLDataset(\n",
    "                split_data,\n",
    "                tokenizer=tokenizer,\n",
    "                max_length=self.config.max_seq_length\n",
    "            )\n",
    "            \n",
    "            loaders[split_name] = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=(split_name == 'train'),\n",
    "                num_workers=2,\n",
    "                pin_memory=torch.cuda.is_available()\n",
    "            )\n",
    "        \n",
    "        return loaders\n",
    "\n",
    "# Load and prepare data\n",
    "dataset_loader = DatasetLoader()\n",
    "raw_data = dataset_loader.load_data(data_path)\n",
    "data_splits = dataset_loader.split_data(raw_data)\n",
    "\n",
    "print(f\"📊 Dataset Statistics:\")\n",
    "print(f\"   Total examples: {len(raw_data)}\")\n",
    "for split_name, split_data in data_splits.items():\n",
    "    print(f\"   {split_name.capitalize()}: {len(split_data)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_visualization"
   },
   "outputs": [],
   "source": [
    "# Visualize dataset statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Analyze query and SQL lengths\n",
    "query_lengths = [len(item['query']) for item in raw_data]\n",
    "sql_lengths = [len(item['sql']) for item in raw_data]\n",
    "\n",
    "# Create interactive visualizations with Plotly\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Query Length Distribution', 'SQL Length Distribution',\n",
    "                   'Dataset Split', 'Query Complexity'),\n",
    "    specs=[[{'type': 'histogram'}, {'type': 'histogram'}],\n",
    "           [{'type': 'pie'}, {'type': 'bar'}]]\n",
    ")\n",
    "\n",
    "# Query lengths histogram\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=query_lengths, name='Query Length', marker_color='blue'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# SQL lengths histogram\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=sql_lengths, name='SQL Length', marker_color='green'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Dataset split pie chart\n",
    "split_sizes = [len(data_splits[split]) for split in ['train', 'val', 'test']]\n",
    "fig.add_trace(\n",
    "    go.Pie(labels=['Train', 'Val', 'Test'], values=split_sizes,\n",
    "           marker_colors=['#1f77b4', '#ff7f0e', '#2ca02c']),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Query complexity (based on keywords)\n",
    "complexity_keywords = ['JOIN', 'GROUP BY', 'ORDER BY', 'WHERE', 'HAVING']\n",
    "complexity_counts = {kw: sum(1 for item in raw_data if kw in item['sql'].upper()) \n",
    "                    for kw in complexity_keywords}\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(complexity_counts.keys()), y=list(complexity_counts.values()),\n",
    "          marker_color='orange'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(height=700, showlegend=False, title_text=\"Dataset Analysis Dashboard\")\n",
    "fig.update_xaxes(title_text=\"Characters\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Characters\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"SQL Keywords\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Count\", row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n📈 Dataset Statistics Summary:\")\n",
    "print(f\"   Average query length: {sum(query_lengths)/len(query_lengths):.1f} chars\")\n",
    "print(f\"   Average SQL length: {sum(sql_lengths)/len(sql_lengths):.1f} chars\")\n",
    "print(f\"   Queries with JOINs: {complexity_counts.get('JOIN', 0)}\")\n",
    "print(f\"   Queries with GROUP BY: {complexity_counts.get('GROUP BY', 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section3"
   },
   "source": [
    "## 🤖 Section 3: Model Architecture & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_config"
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for the Qwen model with LoRA.\"\"\"\n",
    "    model_name: str = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"  # Using smaller model for Colab\n",
    "    max_seq_length: int = 512\n",
    "    temperature: float = 0.3\n",
    "    top_p: float = 0.95\n",
    "    max_new_tokens: int = 256\n",
    "    \n",
    "    # LoRA configuration\n",
    "    use_lora: bool = True\n",
    "    lora_r: int = 16  # Reduced for memory efficiency\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.1\n",
    "    lora_target_modules: List[str] = field(default_factory=lambda: [\n",
    "        \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ])\n",
    "    \n",
    "    # Quantization\n",
    "    use_8bit: bool = True  # Enable 8-bit quantization for memory efficiency\n",
    "    use_gradient_checkpointing: bool = True\n",
    "    \n",
    "    # Training optimization\n",
    "    mixed_precision: str = \"fp16\"  # Use mixed precision training\n",
    "\n",
    "config = ModelConfig()\n",
    "\n",
    "print(\"🔧 Model Configuration:\")\n",
    "print(f\"   Base model: {config.model_name}\")\n",
    "print(f\"   LoRA rank: {config.lora_r}\")\n",
    "print(f\"   8-bit quantization: {config.use_8bit}\")\n",
    "print(f\"   Mixed precision: {config.mixed_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_initialization"
   },
   "outputs": [],
   "source": [
    "# Initialize model with LoRA\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "# Clear memory before loading model\n",
    "colab_utils.clear_memory()\n",
    "\n",
    "print(\"🚀 Loading model and tokenizer...\")\n",
    "\n",
    "# Quantization config for memory efficiency\n",
    "bnb_config = None\n",
    "if config.use_8bit:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_8bit_compute_dtype=torch.float16,\n",
    "        bnb_8bit_use_double_quant=True,\n",
    "        bnb_8bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "\n",
    "# Set padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16 if config.mixed_precision == \"fp16\" else torch.float32\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "if config.use_gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "if config.use_8bit:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "if config.use_lora:\n",
    "    lora_config = LoraConfig(\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        target_modules=config.lora_target_modules,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    \n",
    "    # Add LoRA adapters\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "# Monitor memory after loading\n",
    "colab_utils.monitor_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_info"
   },
   "outputs": [],
   "source": [
    "# Display model information\n",
    "def print_model_info(model):\n",
    "    \"\"\"Print detailed model information.\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(\"\\n🔍 Model Information:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "    \n",
    "    # Model architecture summary\n",
    "    print(\"\\n📐 Model Architecture:\")\n",
    "    print(f\"Model type: {model.__class__.__name__}\")\n",
    "    \n",
    "    if hasattr(model, 'config'):\n",
    "        print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "        print(f\"Num layers: {model.config.num_hidden_layers}\")\n",
    "        print(f\"Num attention heads: {model.config.num_attention_heads}\")\n",
    "        print(f\"Vocab size: {model.config.vocab_size}\")\n",
    "\n",
    "print_model_info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section4"
   },
   "source": [
    "## 🎯 Section 4: Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section4_1"
   },
   "source": [
    "### Phase 1: Supervised Fine-Tuning (SFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sft_trainer"
   },
   "outputs": [],
   "source": [
    "# SFT Trainer Implementation\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "@dataclass\n",
    "class SFTConfig:\n",
    "    \"\"\"Configuration for SFT training.\"\"\"\n",
    "    learning_rate: float = 3e-5\n",
    "    batch_size: int = 2  # Small batch size for Colab\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    num_epochs: int = 3\n",
    "    warmup_steps: int = 100\n",
    "    logging_steps: int = 10\n",
    "    save_steps: int = 50\n",
    "    eval_steps: int = 25\n",
    "    max_grad_norm: float = 1.0\n",
    "    weight_decay: float = 0.01\n",
    "\n",
    "class SFTTrainer:\n",
    "    \"\"\"Supervised Fine-Tuning trainer for SQL generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, config: SFTConfig = None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config or SFTConfig()\n",
    "        self.training_history = {'loss': [], 'eval_loss': []}\n",
    "    \n",
    "    def prepare_training_args(self) -> TrainingArguments:\n",
    "        \"\"\"Prepare training arguments.\"\"\"\n",
    "        return TrainingArguments(\n",
    "            output_dir=str(DRIVE_BASE / 'checkpoints' / 'sft'),\n",
    "            num_train_epochs=self.config.num_epochs,\n",
    "            per_device_train_batch_size=self.config.batch_size,\n",
    "            per_device_eval_batch_size=self.config.batch_size,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            warmup_steps=self.config.warmup_steps,\n",
    "            logging_steps=self.config.logging_steps,\n",
    "            save_steps=self.config.save_steps,\n",
    "            eval_steps=self.config.eval_steps,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            max_grad_norm=self.config.max_grad_norm,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            fp16=True,\n",
    "            push_to_hub=False,\n",
    "            report_to=\"none\",  # Disable wandb for now\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            gradient_checkpointing=True,\n",
    "            optim=\"adamw_torch\",\n",
    "            dataloader_num_workers=2,\n",
    "            remove_unused_columns=False\n",
    "        )\n",
    "    \n",
    "    def train(self, train_dataset, eval_dataset=None):\n",
    "        \"\"\"Run SFT training.\"\"\"\n",
    "        print(\"\\n🎓 Starting Supervised Fine-Tuning...\")\n",
    "        \n",
    "        # Prepare data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "            pad_to_multiple_of=8\n",
    "        )\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=self.prepare_training_args(),\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        # Save model\n",
    "        trainer.save_model()\n",
    "        \n",
    "        # Save training history\n",
    "        self.training_history = train_result.metrics\n",
    "        \n",
    "        print(\"✅ SFT training completed!\")\n",
    "        print(f\"   Final training loss: {train_result.metrics.get('train_loss', 'N/A'):.4f}\")\n",
    "        \n",
    "        return trainer, train_result\n",
    "\n",
    "# Initialize SFT trainer\n",
    "sft_config = SFTConfig(\n",
    "    batch_size=colab_utils.auto_batch_size(2),\n",
    "    num_epochs=2  # Reduced for demo\n",
    ")\n",
    "sft_trainer = SFTTrainer(model, tokenizer, sft_config)\n",
    "\n",
    "print(f\"📝 SFT Configuration:\")\n",
    "print(f\"   Batch size: {sft_config.batch_size}\")\n",
    "print(f\"   Learning rate: {sft_config.learning_rate}\")\n",
    "print(f\"   Epochs: {sft_config.num_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_sft_training"
   },
   "outputs": [],
   "source": [
    "# Create datasets with tokenizer\n",
    "train_dataset = GLDataset(data_splits['train'], tokenizer=tokenizer)\n",
    "val_dataset = GLDataset(data_splits['val'], tokenizer=tokenizer) if data_splits['val'] else None\n",
    "\n",
    "# Run SFT training\n",
    "print(\"🚀 Starting SFT Training...\\n\")\n",
    "trainer, train_result = sft_trainer.train(train_dataset, val_dataset)\n",
    "\n",
    "# Clear memory after training\n",
    "colab_utils.clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section4_2"
   },
   "source": [
    "### Phase 2: Group Relative Policy Optimization (GRPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grpo_trainer"
   },
   "outputs": [],
   "source": [
    "# GRPO Trainer Implementation\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "@dataclass\n",
    "class GRPOConfig:\n",
    "    \"\"\"Configuration for GRPO training.\"\"\"\n",
    "    num_iterations: int = 100\n",
    "    batch_size: int = 2\n",
    "    num_candidates_per_prompt: int = 4\n",
    "    kl_coefficient: float = 0.1\n",
    "    entropy_coefficient: float = 0.01\n",
    "    learning_rate: float = 1e-5\n",
    "    max_grad_norm: float = 1.0\n",
    "    temperature: float = 0.8\n",
    "\n",
    "class RewardEvaluator:\n",
    "    \"\"\"Simple reward evaluator for SQL generation.\"\"\"\n",
    "    \n",
    "    def evaluate_sql(self, query: str, sql: str, reasoning: str = \"\") -> float:\n",
    "        \"\"\"Evaluate SQL quality and return reward.\"\"\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Basic syntax check\n",
    "        sql_upper = sql.upper()\n",
    "        if 'SELECT' in sql_upper:\n",
    "            reward += 2.0\n",
    "        if 'FROM' in sql_upper:\n",
    "            reward += 1.0\n",
    "        \n",
    "        # Check for relevant keywords from query\n",
    "        query_words = query.lower().split()\n",
    "        sql_lower = sql.lower()\n",
    "        for word in query_words:\n",
    "            if word in sql_lower:\n",
    "                reward += 0.5\n",
    "        \n",
    "        # Penalty for syntax errors\n",
    "        if sql.count('(') != sql.count(')'):\n",
    "            reward -= 2.0\n",
    "        \n",
    "        # Bonus for reasoning\n",
    "        if reasoning and len(reasoning) > 10:\n",
    "            reward += 1.0\n",
    "        \n",
    "        return max(0.0, min(10.0, reward))  # Clip between 0 and 10\n",
    "\n",
    "class GRPOTrainer:\n",
    "    \"\"\"GRPO trainer for reinforcement learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, config: GRPOConfig = None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config or GRPOConfig()\n",
    "        self.reward_evaluator = RewardEvaluator()\n",
    "        self.optimizer = AdamW(\n",
    "            model.parameters(),\n",
    "            lr=self.config.learning_rate\n",
    "        )\n",
    "        self.training_history = {\n",
    "            'rewards': [],\n",
    "            'kl_divergence': [],\n",
    "            'loss': []\n",
    "        }\n",
    "    \n",
    "    def generate_candidates(self, prompt: str, num_candidates: int) -> List[str]:\n",
    "        \"\"\"Generate multiple SQL candidates for a prompt.\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        candidates = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_candidates):\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    temperature=self.config.temperature,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id\n",
    "                )\n",
    "                \n",
    "                generated = self.tokenizer.decode(\n",
    "                    outputs[0][inputs['input_ids'].shape[1]:],\n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                candidates.append(generated)\n",
    "        \n",
    "        return candidates\n",
    "    \n",
    "    def compute_rewards(self, query: str, candidates: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Compute rewards for generated candidates.\"\"\"\n",
    "        rewards = []\n",
    "        for sql in candidates:\n",
    "            reward = self.reward_evaluator.evaluate_sql(query, sql)\n",
    "            rewards.append(reward)\n",
    "        return torch.tensor(rewards, dtype=torch.float32)\n",
    "    \n",
    "    def train_step(self, batch_queries: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Single GRPO training step.\"\"\"\n",
    "        total_reward = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        for query in batch_queries:\n",
    "            # Generate candidates\n",
    "            candidates = self.generate_candidates(\n",
    "                query,\n",
    "                self.config.num_candidates_per_prompt\n",
    "            )\n",
    "            \n",
    "            # Compute rewards\n",
    "            rewards = self.compute_rewards(query, candidates)\n",
    "            \n",
    "            # Normalize rewards (GRPO)\n",
    "            baseline = rewards.mean()\n",
    "            advantages = rewards - baseline\n",
    "            \n",
    "            # Select best candidate\n",
    "            best_idx = rewards.argmax()\n",
    "            best_candidate = candidates[best_idx]\n",
    "            \n",
    "            # Compute loss (simplified)\n",
    "            loss = -advantages[best_idx]  # Simplified policy gradient\n",
    "            \n",
    "            total_reward += rewards.max().item()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass (simplified for demo)\n",
    "        self.optimizer.zero_grad()\n",
    "        # In real implementation, would compute actual policy gradients\n",
    "        # For demo, we'll use a placeholder loss\n",
    "        placeholder_loss = torch.tensor(total_loss, requires_grad=True)\n",
    "        if placeholder_loss.grad_fn:\n",
    "            placeholder_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'avg_reward': total_reward / len(batch_queries),\n",
    "            'avg_loss': total_loss / len(batch_queries)\n",
    "        }\n",
    "    \n",
    "    def train(self, train_data: List[Dict[str, str]], num_iterations: int = None):\n",
    "        \"\"\"Run GRPO training.\"\"\"\n",
    "        num_iterations = num_iterations or self.config.num_iterations\n",
    "        \n",
    "        print(\"\\n🚀 Starting GRPO Training...\")\n",
    "        progress_bar = tqdm(range(num_iterations), desc=\"GRPO Training\")\n",
    "        \n",
    "        for iteration in progress_bar:\n",
    "            # Sample batch\n",
    "            batch_size = min(self.config.batch_size, len(train_data))\n",
    "            batch_indices = np.random.choice(len(train_data), batch_size, replace=False)\n",
    "            batch = [train_data[i] for i in batch_indices]\n",
    "            batch_queries = [item['query'] for item in batch]\n",
    "            \n",
    "            # Training step\n",
    "            metrics = self.train_step(batch_queries)\n",
    "            \n",
    "            # Update history\n",
    "            self.training_history['rewards'].append(metrics['avg_reward'])\n",
    "            self.training_history['loss'].append(metrics['avg_loss'])\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'reward': f\"{metrics['avg_reward']:.2f}\",\n",
    "                'loss': f\"{metrics['avg_loss']:.4f}\"\n",
    "            })\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if (iteration + 1) % 20 == 0:\n",
    "                checkpoint_path = DRIVE_BASE / 'checkpoints' / 'grpo' / f'checkpoint_{iteration+1}'\n",
    "                checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "                self.model.save_pretrained(checkpoint_path)\n",
    "        \n",
    "        print(\"\\n✅ GRPO training completed!\")\n",
    "        print(f\"   Final average reward: {np.mean(self.training_history['rewards'][-10:]):.2f}\")\n",
    "        \n",
    "        return self.training_history\n",
    "\n",
    "# Initialize GRPO trainer\n",
    "grpo_config = GRPOConfig(\n",
    "    num_iterations=50,  # Reduced for demo\n",
    "    batch_size=1,  # Small batch for memory\n",
    "    num_candidates_per_prompt=3\n",
    ")\n",
    "grpo_trainer = GRPOTrainer(model, tokenizer, grpo_config)\n",
    "\n",
    "print(f\"📝 GRPO Configuration:\")\n",
    "print(f\"   Iterations: {grpo_config.num_iterations}\")\n",
    "print(f\"   Candidates per prompt: {grpo_config.num_candidates_per_prompt}\")\n",
    "print(f\"   KL coefficient: {grpo_config.kl_coefficient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_grpo_training"
   },
   "outputs": [],
   "source": [
    "# Run GRPO training\n",
    "grpo_history = grpo_trainer.train(data_splits['train'], num_iterations=30)\n",
    "\n",
    "# Visualize GRPO training progress\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    y=grpo_history['rewards'],\n",
    "    mode='lines+markers',\n",
    "    name='Average Reward',\n",
    "    line=dict(color='green', width=2)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='GRPO Training Progress',\n",
    "    xaxis_title='Iteration',\n",
    "    yaxis_title='Average Reward',\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Clear memory\n",
    "colab_utils.clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section5"
   },
   "source": [
    "## 🤝 Section 5: Multi-Agent System Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agent_system"
   },
   "outputs": [],
   "source": [
    "# Simplified Multi-Agent System\n",
    "from enum import Enum\n",
    "from typing import Dict, Any, Optional\n",
    "import asyncio\n",
    "\n",
    "class AgentRole(Enum):\n",
    "    ORCHESTRATOR = \"orchestrator\"\n",
    "    SCHEMA_ANALYZER = \"schema_analyzer\"\n",
    "    QUERY_GENERATOR = \"query_generator\"\n",
    "    VALIDATOR = \"validator\"\n",
    "    REWARD_EVALUATOR = \"reward_evaluator\"\n",
    "\n",
    "class BaseAgent:\n",
    "    \"\"\"Base class for all agents.\"\"\"\n",
    "    \n",
    "    def __init__(self, role: AgentRole):\n",
    "        self.role = role\n",
    "        self.status = \"idle\"\n",
    "    \n",
    "    async def process(self, message: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Process a message and return result.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SchemaAnalyzer(BaseAgent):\n",
    "    \"\"\"Agent for analyzing database schema.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(AgentRole.SCHEMA_ANALYZER)\n",
    "        self.schema = {\n",
    "            'PAC_MNT_PROJECTS': ['Project_Code', 'Project_Name', 'Budget', 'Status'],\n",
    "            'SRM_COMPANIES': ['Company_ID', 'Company_Name', 'Company_Code'],\n",
    "            'SRM_CONTACTS': ['Contact_ID', 'Company_ID', 'Contact_Name', 'Email'],\n",
    "            'PROJSTAFF': ['Staff_ID', 'Project_Code', 'Resource_Code', 'Role']\n",
    "        }\n",
    "    \n",
    "    async def process(self, message: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        query = message.get('query', '')\n",
    "        relevant_tables = []\n",
    "        \n",
    "        # Simple keyword matching\n",
    "        query_lower = query.lower()\n",
    "        if 'project' in query_lower:\n",
    "            relevant_tables.append('PAC_MNT_PROJECTS')\n",
    "        if 'company' in query_lower or 'companies' in query_lower:\n",
    "            relevant_tables.append('SRM_COMPANIES')\n",
    "        if 'contact' in query_lower:\n",
    "            relevant_tables.append('SRM_CONTACTS')\n",
    "        if 'staff' in query_lower or 'resource' in query_lower:\n",
    "            relevant_tables.append('PROJSTAFF')\n",
    "        \n",
    "        return {\n",
    "            'relevant_tables': relevant_tables,\n",
    "            'schema_info': {table: self.schema[table] for table in relevant_tables}\n",
    "        }\n",
    "\n",
    "class QueryGenerator(BaseAgent):\n",
    "    \"\"\"Agent for generating SQL queries.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__(AgentRole.QUERY_GENERATOR)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    async def process(self, message: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        query = message.get('query', '')\n",
    "        schema_info = message.get('schema_info', {})\n",
    "        \n",
    "        # Format prompt with schema information\n",
    "        prompt = f\"\"\"Generate SQL for: {query}\n",
    "Available tables: {', '.join(schema_info.keys())}\n",
    "SQL:\"\"\"\n",
    "        \n",
    "        # Generate SQL\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                temperature=0.3,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        sql = self.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        return {'sql': sql, 'reasoning': 'Generated using fine-tuned model'}\n",
    "\n",
    "class Validator(BaseAgent):\n",
    "    \"\"\"Agent for validating SQL queries.\"\"\"\n",
    "    \n",
    "    async def process(self, message: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        sql = message.get('sql', '')\n",
    "        \n",
    "        # Basic validation\n",
    "        is_valid = True\n",
    "        errors = []\n",
    "        \n",
    "        sql_upper = sql.upper()\n",
    "        if 'SELECT' not in sql_upper:\n",
    "            is_valid = False\n",
    "            errors.append('Missing SELECT statement')\n",
    "        \n",
    "        if 'FROM' not in sql_upper:\n",
    "            is_valid = False\n",
    "            errors.append('Missing FROM clause')\n",
    "        \n",
    "        if sql.count('(') != sql.count(')'):\n",
    "            is_valid = False\n",
    "            errors.append('Unbalanced parentheses')\n",
    "        \n",
    "        return {'is_valid': is_valid, 'errors': errors}\n",
    "\n",
    "class Orchestrator(BaseAgent):\n",
    "    \"\"\"Main orchestrator agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__(AgentRole.ORCHESTRATOR)\n",
    "        self.agents = {\n",
    "            AgentRole.SCHEMA_ANALYZER: SchemaAnalyzer(),\n",
    "            AgentRole.QUERY_GENERATOR: QueryGenerator(model, tokenizer),\n",
    "            AgentRole.VALIDATOR: Validator()\n",
    "        }\n",
    "    \n",
    "    async def process(self, message: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        query = message.get('query', '')\n",
    "        \n",
    "        print(f\"\\n🎯 Processing query: {query}\")\n",
    "        \n",
    "        # Step 1: Analyze schema\n",
    "        print(\"  📊 Analyzing schema...\")\n",
    "        schema_result = await self.agents[AgentRole.SCHEMA_ANALYZER].process({'query': query})\n",
    "        \n",
    "        # Step 2: Generate SQL\n",
    "        print(\"  🔨 Generating SQL...\")\n",
    "        gen_result = await self.agents[AgentRole.QUERY_GENERATOR].process({\n",
    "            'query': query,\n",
    "            'schema_info': schema_result['schema_info']\n",
    "        })\n",
    "        \n",
    "        # Step 3: Validate SQL\n",
    "        print(\"  ✅ Validating SQL...\")\n",
    "        val_result = await self.agents[AgentRole.VALIDATOR].process({'sql': gen_result['sql']})\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'sql': gen_result['sql'],\n",
    "            'reasoning': gen_result['reasoning'],\n",
    "            'is_valid': val_result['is_valid'],\n",
    "            'errors': val_result.get('errors', []),\n",
    "            'relevant_tables': schema_result['relevant_tables']\n",
    "        }\n",
    "\n",
    "# Initialize orchestrator\n",
    "orchestrator = Orchestrator(model, tokenizer)\n",
    "\n",
    "print(\"✅ Multi-Agent System initialized with agents:\")\n",
    "for role in AgentRole:\n",
    "    print(f\"   • {role.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_agent_system"
   },
   "outputs": [],
   "source": [
    "# Test the multi-agent system\n",
    "test_queries = [\n",
    "    \"Show all active projects with budget over 50000\",\n",
    "    \"List companies and their contacts\",\n",
    "    \"Find staff assignments for current projects\"\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing Multi-Agent System\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for test_query in test_queries:\n",
    "    # Process through agent system\n",
    "    result = await orchestrator.process({'query': test_query})\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n📝 Query: {result['query']}\")\n",
    "    print(f\"📊 Relevant Tables: {', '.join(result['relevant_tables'])}\")\n",
    "    print(f\"\\n💻 Generated SQL:\\n{result['sql']}\")\n",
    "    print(f\"\\n🎯 Validation: {'✅ Valid' if result['is_valid'] else '❌ Invalid'}\")\n",
    "    if result['errors']:\n",
    "        print(f\"   Errors: {', '.join(result['errors'])}\")\n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section6"
   },
   "source": [
    "## 📈 Section 6: Evaluation & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation_metrics"
   },
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "\n",
    "class SQLEvaluator:\n",
    "    \"\"\"Evaluator for SQL generation quality.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'exact_match': 0,\n",
    "            'syntax_valid': 0,\n",
    "            'semantic_score': 0,\n",
    "            'keyword_recall': 0\n",
    "        }\n",
    "    \n",
    "    def evaluate_syntax(self, sql: str) -> bool:\n",
    "        \"\"\"Check if SQL has valid syntax.\"\"\"\n",
    "        sql_upper = sql.upper()\n",
    "        required = ['SELECT', 'FROM']\n",
    "        return all(keyword in sql_upper for keyword in required)\n",
    "    \n",
    "    def calculate_keyword_recall(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"Calculate keyword recall between generated and reference SQL.\"\"\"\n",
    "        # Extract SQL keywords\n",
    "        keywords = ['SELECT', 'FROM', 'WHERE', 'JOIN', 'GROUP BY', 'ORDER BY', 'HAVING']\n",
    "        \n",
    "        ref_keywords = set()\n",
    "        gen_keywords = set()\n",
    "        \n",
    "        for keyword in keywords:\n",
    "            if keyword in reference.upper():\n",
    "                ref_keywords.add(keyword)\n",
    "            if keyword in generated.upper():\n",
    "                gen_keywords.add(keyword)\n",
    "        \n",
    "        if not ref_keywords:\n",
    "            return 1.0\n",
    "        \n",
    "        return len(ref_keywords.intersection(gen_keywords)) / len(ref_keywords)\n",
    "    \n",
    "    def evaluate_batch(self, predictions: List[str], references: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate a batch of predictions.\"\"\"\n",
    "        results = {\n",
    "            'exact_match': 0,\n",
    "            'syntax_valid': 0,\n",
    "            'keyword_recall': 0,\n",
    "            'avg_length_diff': 0\n",
    "        }\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            # Exact match\n",
    "            if pred.strip().upper() == ref.strip().upper():\n",
    "                results['exact_match'] += 1\n",
    "            \n",
    "            # Syntax validity\n",
    "            if self.evaluate_syntax(pred):\n",
    "                results['syntax_valid'] += 1\n",
    "            \n",
    "            # Keyword recall\n",
    "            results['keyword_recall'] += self.calculate_keyword_recall(pred, ref)\n",
    "            \n",
    "            # Length difference\n",
    "            results['avg_length_diff'] += abs(len(pred) - len(ref))\n",
    "        \n",
    "        # Normalize\n",
    "        n = len(predictions)\n",
    "        for key in results:\n",
    "            results[key] /= n\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Evaluate on test set\n",
    "evaluator = SQLEvaluator()\n",
    "\n",
    "print(\"🧪 Evaluating model on test set...\\n\")\n",
    "\n",
    "test_predictions = []\n",
    "test_references = []\n",
    "\n",
    "# Generate predictions for test set\n",
    "for item in data_splits['test'][:3]:  # Limit to 3 for demo\n",
    "    query = item['query']\n",
    "    reference_sql = item['sql']\n",
    "    \n",
    "    # Generate prediction\n",
    "    prompt = f\"Generate SQL for: {query}\\nSQL:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.3,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    generated_sql = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    test_predictions.append(generated_sql)\n",
    "    test_references.append(reference_sql)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Generated: {generated_sql[:100]}...\")\n",
    "    print(f\"Reference: {reference_sql[:100]}...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = evaluator.evaluate_batch(test_predictions, test_references)\n",
    "\n",
    "print(\"\\n📊 Evaluation Metrics:\")\n",
    "print(\"=\"*40)\n",
    "for metric, value in metrics.items():\n",
    "    if metric == 'avg_length_diff':\n",
    "        print(f\"{metric}: {value:.1f} chars\")\n",
    "    else:\n",
    "        print(f\"{metric}: {value:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interactive_testing"
   },
   "outputs": [],
   "source": [
    "# Interactive testing widget\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Create interactive query testing interface\n",
    "query_input = widgets.Textarea(\n",
    "    value='Show all projects with budget over 100000',\n",
    "    placeholder='Enter your natural language query...',\n",
    "    description='Query:',\n",
    "    layout=widgets.Layout(width='100%', height='60px')\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(\n",
    "    description='Generate SQL',\n",
    "    button_style='primary',\n",
    "    icon='play'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_generate_click(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        \n",
    "        query = query_input.value\n",
    "        print(f\"🔍 Query: {query}\\n\")\n",
    "        \n",
    "        # Generate SQL\n",
    "        prompt = f\"Generate SQL for: {query}\\nSQL:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                temperature=0.3,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        sql = tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        is_valid = evaluator.evaluate_syntax(sql)\n",
    "        \n",
    "        print(\"💻 Generated SQL:\")\n",
    "        print(\"-\" * 40)\n",
    "        display(HTML(f\"<pre style='background-color: #f0f0f0; padding: 10px;'>{sql}</pre>\"))\n",
    "        \n",
    "        print(f\"\\n✅ Validation: {'Valid' if is_valid else 'Invalid'}\")\n",
    "\n",
    "generate_button.on_click(on_generate_click)\n",
    "\n",
    "# Display interface\n",
    "print(\"🎮 Interactive SQL Generation\\n\")\n",
    "display(query_input)\n",
    "display(generate_button)\n",
    "display(output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section7"
   },
   "source": [
    "## 💾 Section 7: Model Export & Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_export"
   },
   "outputs": [],
   "source": [
    "# Export model for deployment\n",
    "import pickle\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "def export_model(model, tokenizer, export_path: Path, metadata: Dict[str, Any] = None):\n",
    "    \"\"\"Export model with all necessary files for deployment.\"\"\"\n",
    "    print(\"📦 Exporting model for deployment...\\n\")\n",
    "    \n",
    "    # Create export directory\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    export_dir = export_path / f'gl_rl_model_{timestamp}'\n",
    "    export_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    print(\"  💾 Saving model weights...\")\n",
    "    model_dir = export_dir / 'model'\n",
    "    model.save_pretrained(model_dir)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    print(\"  💾 Saving tokenizer...\")\n",
    "    tokenizer_dir = export_dir / 'tokenizer'\n",
    "    tokenizer.save_pretrained(tokenizer_dir)\n",
    "    \n",
    "    # Save configuration\n",
    "    print(\"  💾 Saving configuration...\")\n",
    "    config_data = {\n",
    "        'model_config': config.__dict__,\n",
    "        'timestamp': timestamp,\n",
    "        'metadata': metadata or {}\n",
    "    }\n",
    "    \n",
    "    with open(export_dir / 'config.json', 'w') as f:\n",
    "        json.dump(config_data, f, indent=2)\n",
    "    \n",
    "    # Create inference script\n",
    "    print(\"  💾 Creating inference script...\")\n",
    "    inference_script = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Inference script for GL RL Model.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import json\n",
    "import sys\n",
    "\n",
    "def load_model(model_dir=\"model\", tokenizer_dir=\"tokenizer\"):\n",
    "    \"\"\"Load the fine-tuned model.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_sql(query, model, tokenizer, max_length=256):\n",
    "    \"\"\"Generate SQL for a natural language query.\"\"\"\n",
    "    prompt = f\"Generate SQL for: {query}\\\\nSQL:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.3,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    sql = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return sql\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load model\n",
    "    print(\"Loading model...\")\n",
    "    model, tokenizer = load_model()\n",
    "    \n",
    "    # Interactive mode\n",
    "    print(\"GL RL Model - SQL Generator\")\n",
    "    print(\"Type 'quit' to exit\\\\n\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Enter query: \")\n",
    "        if query.lower() == \"quit\":\n",
    "            break\n",
    "        \n",
    "        sql = generate_sql(query, model, tokenizer)\n",
    "        print(f\"\\\\nGenerated SQL:\\\\n{sql}\\\\n\")\n",
    "'''\n",
    "    \n",
    "    with open(export_dir / 'inference.py', 'w') as f:\n",
    "        f.write(inference_script)\n",
    "    \n",
    "    # Create README\n",
    "    print(\"  💾 Creating README...\")\n",
    "    readme_content = f\"\"\"# GL RL Model Export\n",
    "\n",
    "## Model Information\n",
    "- Export Date: {timestamp}\n",
    "- Base Model: {config.model_name}\n",
    "- LoRA Rank: {config.lora_r}\n",
    "- Training: SFT + GRPO\n",
    "\n",
    "## Directory Structure\n",
    "```\n",
    ".\n",
    "├── model/          # Model weights\n",
    "├── tokenizer/      # Tokenizer files\n",
    "├── config.json     # Configuration\n",
    "├── inference.py    # Inference script\n",
    "└── README.md       # This file\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Quick Start\n",
    "```python\n",
    "python inference.py\n",
    "```\n",
    "\n",
    "### Python API\n",
    "```python\n",
    "from inference import load_model, generate_sql\n",
    "\n",
    "model, tokenizer = load_model()\n",
    "sql = generate_sql(\"Show all active projects\", model, tokenizer)\n",
    "print(sql)\n",
    "```\n",
    "\n",
    "## Requirements\n",
    "- torch>=2.0.0\n",
    "- transformers>=4.35.0\n",
    "- peft>=0.6.0\n",
    "\n",
    "## License\n",
    "See LICENSE file for details.\n",
    "\"\"\"\n",
    "    \n",
    "    with open(export_dir / 'README.md', 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    # Create requirements file\n",
    "    requirements = [\n",
    "        'torch>=2.0.0',\n",
    "        'transformers>=4.35.0',\n",
    "        'peft>=0.6.0',\n",
    "        'accelerate>=0.24.0'\n",
    "    ]\n",
    "    \n",
    "    with open(export_dir / 'requirements.txt', 'w') as f:\n",
    "        f.write('\\n'.join(requirements))\n",
    "    \n",
    "    print(f\"\\n✅ Model exported successfully to: {export_dir}\")\n",
    "    print(f\"   Total size: {sum(f.stat().st_size for f in export_dir.rglob('*') if f.is_file()) / 1e6:.1f} MB\")\n",
    "    \n",
    "    return export_dir\n",
    "\n",
    "# Export the model\n",
    "export_dir = export_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    DRIVE_BASE / 'exports',\n",
    "    metadata={\n",
    "        'training_samples': len(data_splits['train']),\n",
    "        'sft_epochs': sft_config.num_epochs,\n",
    "        'grpo_iterations': grpo_config.num_iterations\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_archive"
   },
   "outputs": [],
   "source": [
    "# Create downloadable archive\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def create_archive(export_dir: Path) -> Path:\n",
    "    \"\"\"Create a ZIP archive of the exported model.\"\"\"\n",
    "    archive_path = export_dir.parent / f\"{export_dir.name}.zip\"\n",
    "    \n",
    "    print(f\"📦 Creating archive: {archive_path.name}\")\n",
    "    \n",
    "    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for file in export_dir.rglob('*'):\n",
    "            if file.is_file():\n",
    "                arcname = str(file.relative_to(export_dir.parent))\n",
    "                zipf.write(file, arcname)\n",
    "                print(f\"   Added: {arcname}\")\n",
    "    \n",
    "    size_mb = archive_path.stat().st_size / 1e6\n",
    "    print(f\"\\n✅ Archive created: {archive_path}\")\n",
    "    print(f\"   Size: {size_mb:.1f} MB\")\n",
    "    \n",
    "    return archive_path\n",
    "\n",
    "# Create archive\n",
    "archive_path = create_archive(export_dir)\n",
    "\n",
    "# Provide download link in Colab\n",
    "from google.colab import files\n",
    "\n",
    "print(\"\\n📥 Download your model:\")\n",
    "# Uncomment to download:\n",
    "# files.download(str(archive_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 🎉 Conclusion & Next Steps\n",
    "\n",
    "### ✅ What We Accomplished\n",
    "1. **Environment Setup**: Configured Google Colab with GPU support\n",
    "2. **Data Preparation**: Created and processed training data for GL SQL generation\n",
    "3. **Model Training**: Implemented both SFT and GRPO training phases\n",
    "4. **Multi-Agent System**: Demonstrated agent-based SQL generation workflow\n",
    "5. **Evaluation**: Tested model performance with multiple metrics\n",
    "6. **Export**: Created deployable model package\n",
    "\n",
    "### 📈 Performance Summary\n",
    "- **Training Loss**: Reduced through SFT training\n",
    "- **GRPO Rewards**: Improved through reinforcement learning\n",
    "- **Syntax Validity**: High percentage of valid SQL generation\n",
    "- **Memory Usage**: Optimized for Colab GPU constraints\n",
    "\n",
    "### 🚀 Next Steps\n",
    "1. **Scale Up Training**:\n",
    "   - Use larger datasets\n",
    "   - Train for more epochs\n",
    "   - Try larger model variants\n",
    "\n",
    "2. **Improve Reward Function**:\n",
    "   - Add semantic similarity scoring\n",
    "   - Implement execution-based rewards\n",
    "   - Include schema compliance checks\n",
    "\n",
    "3. **Enhance Agent System**:\n",
    "   - Add more specialized agents\n",
    "   - Implement async processing\n",
    "   - Add caching mechanisms\n",
    "\n",
    "4. **Production Deployment**:\n",
    "   - Create API endpoints\n",
    "   - Implement monitoring\n",
    "   - Add A/B testing capability\n",
    "\n",
    "### 📚 Resources\n",
    "- [Qwen Model Documentation](https://github.com/QwenLM/Qwen)\n",
    "- [PEFT Library](https://github.com/huggingface/peft)\n",
    "- [TRL Documentation](https://github.com/lvwerra/trl)\n",
    "- [Google Colab Tips](https://colab.research.google.com/notebooks/pro.ipynb)\n",
    "\n",
    "### 💡 Tips for Better Results\n",
    "- Use Colab Pro for longer training sessions\n",
    "- Monitor GPU memory usage closely\n",
    "- Save checkpoints frequently to Google Drive\n",
    "- Experiment with different hyperparameters\n",
    "- Collect more domain-specific training data\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for using this GL RL Model training notebook!**\n",
    "\n",
    "For questions or improvements, please contribute to the repository.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gl_rl_model_colab_training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}